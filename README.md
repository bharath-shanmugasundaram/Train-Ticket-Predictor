# ğŸš† Train Ticket Booking Prediction â€” Deep Neural Network (NumPy From Scratch)

This project builds a **5-layer deep neural network** completely from scratch using **NumPy**, designed to predict whether a **train ticket booking** will be successful or not. The dataset involves several categorical and numerical fields, all of which are transformed manually before training the neural network.

The project demonstrates how deep learning works internally:
âœ” Manual feature preprocessing
âœ” Multi-layer forward propagation
âœ” Multi-layer backpropagation
âœ” Gradient descent optimization
âœ” Training loss monitoring
âœ” Manual binary classification inference

Everything is implemented **without TensorFlow, PyTorch, or Scikit-learn**.

---

# ğŸ“Š Dataset Overview

Dataset used:
**\`train_ticket_booking_dataset_50000.csv\`**

It contains **50,000+ rows** with the following features:

| Feature | Description |
|--------|-------------|
| Day_of_Week | Day of booking |
| Time_of_Booking | Morning / Afternoon / Evening / Night |
| Train_Popularity | Low / Medium / High |
| Season | Normal / Holiday / Festival |
| Travel_Class | Sleeper / 3AC / 2AC / 1AC |
| Booking_Type | Tatkal / Normal |
| Booking_Status | Final output label (0 = Fail, 1 = Success) |

---

# ğŸ”„ Data Preprocessing (Manual Encoding)

All categorical values are mapped manually to integers:

### **Day\_of\_Week**
Mon â†’ 2
Tue â†’ 3
Wed â†’ 4
Thu â†’ 5
Fri â†’ 6
Sat â†’ 7
Sun â†’ 1

### **Time\_of\_Booking**
Morning â†’ 1
Afternoon â†’ 2
Evening â†’ 3
Night â†’ 4

### **Train\_Popularity**
Low â†’ 1
Medium â†’ 2
High â†’ 3

### **Season**
Normal â†’ 1
Holiday â†’ 2
Festival â†’ 3

### **Travel\_Class**
Sleeper â†’ 1
3AC â†’ 2
2AC â†’ 3
1AC â†’ 4

### **Booking\_Type**
Tatkal â†’ 1
Normal â†’ 2

### **Splitting the Dataset**
- First **50,000** rows â†’ **Training set**
- Remaining rows â†’ **Test set**, saved as \`Test\_dataset.csv\`

---

# ğŸ§  Neural Network Architecture

The network contains **5 hidden layers**, implemented with plain NumPy:

| Layer | Neurons | Activation |
|-------|------------|-------------|
| Input Layer | 13 features | â€” |
| Hidden Layer 1 | 13 neurons | Sigmoid |
| Hidden Layer 2 | 10 neurons | Sigmoid |
| Hidden Layer 3 | 7 neurons | Sigmoid |
| Hidden Layer 4 | 3 neurons | Sigmoid |
| Output Layer | 1 neuron | Sigmoid |

The final output is binary:
- \`0\` â†’ Booking unsuccessful
- \`1\` â†’ Booking successful

---

# ğŸ§® Activation Functions

## Sigmoid
\[
\sigma(z) = \frac{1}{1 + e^{-z}}
\]

## Sigmoid Derivative
\[
\sigma'(a) = a(1 - a)
\]

---

# ğŸ”¢ Forward Propagation

Forward pass executes in this order:
Input X
â†“
Z1 = W1Â·X + b1
A1 = sigmoid(Z1)
â†“
Z2 = W2Â·A1 + b2
A2 = sigmoid(Z2)
â†“
Z3 = W3Â·A2 + b3
A3 = sigmoid(Z3)
â†“
Z4 = W4Â·A3 + b4
A4 = sigmoid(Z4)
â†“
Z5 = W5Â·A4 + b5
A5 = sigmoid(Z5) â† Final output

---

# ğŸ”„ Backpropagation

Error is propagated back through all 5 layers using:

- \`dZ5\`, \`dW5\`, \`db5\`
- \`dZ4\`, \`dW4\`, \`db4\`
- \`dZ3\`, \`dW3\`, \`db3\`
- \`dZ2\`, \`dW2\`, \`db2\`
- \`dZ1\`, \`dW1\`, \`db1\`

### Gradient Descent Update:
\[
W := W - \alpha \cdot dW
\]
\[
b := b - \alpha \cdot db
\]

Where **Î± = learning rate**.

---

# âš™ï¸ Training Configuration

| Hyperparameter | Value |
|----------------|--------|
| Epochs | 10,000 |
| Learning Rate | 0.1 |
| Batch Size | 64 |
| Loss function | Binary Cross-Entropy |
| Activation | Sigmoid (all layers) |

Loss is recorded at each epoch and stored in \`cost\`.

---

# ğŸ“‰ Loss Visualization

Loss curve is plotted using:

\`\`\`python
sns.lineplot(cost)
\`\`\`
This shows whether the model is converging properly.

---

# ğŸ§ª Prediction Pipeline

Once trained, each test sample is passed through all 5 layers.
Binary classification rule:

If A5 >= 0.5 â†’ Predict 1
Else â†’ Predict 0

This is done for all rows in the test dataset to compute accuracy.

---

# ğŸ¯ Final Accuracy

Accuracy is calculated using:

$$
\text{Accuracy} = \frac{\text{correct\_predictions}}{\text{total\_test\_samples}} \times 100
$$

You will see output like:

\`\`\`
Accuracy : 87.24%
\`\`\`
(Your accuracy may vary depending on initialization and randomness.)

---

# ğŸ“¦ Requirements

Install required libraries:

\`\`\`
pip install numpy pandas seaborn
\`\`\`

---

# â–¶ï¸ How to Run

To execute the complete training pipeline:

**Option 1 â€” Run as Python script**
\`\`\`
python train_model.py
\`\`\`

**Option 2 â€” Run Jupyter Notebook / VSCode**
Execute all cells in order.

---

# ğŸ“˜ What You Will Learn

By studying this project, you will understand:

* How to **manually encode categorical ML dataset features**
* How to build a **deep feedforward neural network from scratch**
* How **forward propagation** works mathematically
* How **backpropagation** works layer-by-layer
* How to apply **gradient descent** without ML libraries
* How to measure accuracy of a classifier
* How to visualize loss curves
* How real-world categorical data is transformed for machine learning

This project is ideal for students learning **deep learning fundamentals**.

---

# ğŸ‘¨â€ğŸ’» Author

**Bharath**
Machine Learning Developer
Focused on understanding and building ML systems from scratch to gain deep foundational knowledge.
